# :abacus: Plan de données `data plane` 

:round_pushpin: Joindre un noeud 

À l'initialisation du plan de contrôle, un :tickets: jeton à été donné avec la commande `kubeadm`, récupérer cette commande.

Elle devrait ressembler à la suivante:

```
$ kubeadm join betelgeuse.orion.gasy.africa:6443 --token 4gp39y.898okq2rcj3j8wgl \
    --discovery-token-ca-cert-hash sha256:348cf90011e40088944a5f5cfe3279c04a0dfb24f56ba21209e61fdc15af3645
```
```yaml
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.

```

:bulb: N'oubliez de rajouter `sudo` 

## [:back:](../#abacus-les-plan-de-données-data-plane)


## :cl: Gestion des Jetons :tickets: 

Si vous avez perdu le :tickets: jeton, ou que le jeton a expiré

:round_pushpin: Sur le :control_knobs: plan de contrôle, 

- [ ] Afficher la liste des :tickets: Jetons

Observer le champ `TTL` (Time To Live), il indique le temps restant avant expiration du jeton d'une durée de vie de 24h

```
kubeadm token list
```
```yaml
TOKEN                     TTL         EXPIRES                USAGES                   DESCRIPTION                                                EXTRA GROUPS
rrmied.zty2u72rw09jm8da   23h         2023-04-11T21:25:21Z   authentication,signing   The default bootstrap token generated by 'kubeadm init'.   system:bootstrappers:kubeadm:default-node-token
```

:star: Si le jeton n'a pas expiré:

:control_knobs: Sur le plan de contrôle, récuperer les information suivantes

- [ ] Le hashage :hash: du certificat `SHA256`

```
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt \
    | openssl rsa -pubin -outform der 2>/dev/null \
    | openssl dgst -sha256 -hex \
    | sed 's/^.* //'
```
> 2ac9eca4748cb94db31f3f271bf49d3149287bb664050a82bf8dc97bf673daa0

- [ ] Le jeton :tickets: non expiré

```
$ kubeadm token list
```

:abacus: Sur le plan de données

- [ ] Donner les valeurs récupérées aux variables d'environnements suivantes: (par example)

```
CTL_PLANE="betelgeuse.orion.gasy.africa" ; \
TOKEN="rrmied.zty2u72rw09jm8da" ; \
CA_CERT_HASH="2ac9eca4748cb94db31f3f271bf49d3149287bb664050a82bf8dc97bf673daa0"
```

- [ ] Exécuter la commande permettant de joindre la grappe

```
sudo kubeadm join ${CTL_PLANE}:6443 --token ${TOKEN} --discovery-token-ca-cert-hash sha256:${CA_CERT_HASH}
```

:round_pushpin: Si les jetons ont tous expirés, regénérer un jeton avec les commandes ci-dessous

:control_knobs: sur le plan de controle

```
$ kubeadm token create --print-join-command

```
> Retourne
```
W0325 19:10:34.388061   53964 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]
kubeadm join betelgeuse.orion.gasy.africa:6443 --token fu6544.o36km11eg95slej1     --discovery-token-ca-cert-hash sha256:348cf90011e40088944a5f5cfe3279c04a0dfb24f56ba21209e61fdc15af3645
```

:abacus: sur le plan de données

```
$ sudo kubeadm join betelgeuse.orion.gasy.africa:6443 --token fu6544.o36km11eg95slej1     --discovery-token-ca-cert-hash sha256:348cf90011e40088944a5f5cfe3279c04a0dfb24f56ba21209e61fdc15af3645 
```

## :x: Troubleshooting

:abacus: Sur le plan de données

- [ ] [Container runtime network not ready: cni config uninitialized](https://stackoverflow.com/questions/49112336/container-runtime-network-not-ready-cni-config-uninitialized)

* Enlever le service de sécurité [`apparmor`]([https://www.apparmor.net/](https://kubernetes.io/docs/tutorials/security/apparmor/))

```
systemctl stop apparmor ; systemctl disable apparmor ; systemctl restart containerd.service
```

:control_knobs: Sur le plan de controle

Lister les noeuds and obtenir le <nom-du-neoud> vous voulez drainer (enlever de la grappe)


```
$ kubectl get nodes
```

1) Pemièrement, drainer le noeud

```
$ kubectl drain <nom-du-neoud>
```

Vous pouvez ignorer les `daemonsets` et données locale dans la machine

```
$ kubectl drain <node-name> --ignore-daemonsets --delete-local-data
```
  
2) Finalement, enlever le noeud

```
$ kubectl delete node <node-name>
```
  
#### :recycle: Réinitialiser le noeud

- [ ] Pre-fligth `/etc/kubernetes/kubelet.conf already exists`
    
```
sudo kubeadm join ${CTL_PLANE}:6443 --token ${TOKEN} --discovery-token-ca-cert-hash sha256:${CA_CERT_HASH}
```
```yaml
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
        [ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
```

- [ ] Réinitialiser
    
```
 sudo kubeadm reset
```
> Retourne
```
[reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0325 18:59:42.226203 3662407 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] No etcd config found. Assuming external etcd
[reset] Please, manually reset etcd to prevent further issues
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
  
```
  
- [ ] Nettoyer les fichiers restants 

```
   sudo rm -rf /etc/cni $HOME/.kube/config
```
  
:interrobang: Unhealthy :droplet: Kubelet
    
```
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'
error execution phase kubelet-start: timed out waiting for the condition
To see the stack trace of this error execute with --v=5 or higher
```
    
# References

https://stackoverflow.com/questions/51126164/how-do-i-find-the-join-command-for-kubeadm-on-the-master

https://unix.stackexchange.com/questions/87405/how-can-i-execute-local-script-on-remote-machine-and-include-arguments

https://sdorsett.github.io/post/2018-12-26-using-local-exec-and-remote-exec-provisioners-with-terraform/

https://stackoverflow.com/questions/35757620/how-to-gracefully-remove-a-node-from-kubernetes

https://stackoverflow.com/questions/35757620/how-to-gracefully-remove-a-node-from-kubernetes
